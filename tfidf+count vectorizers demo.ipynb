{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_vectorizer.tokenizers import BaseTokenizer, Tokenizer\n",
    "from text_vectorizer.vectorizers import CountVectorizer, TfIdfVectorizer\n",
    "import string\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(' ')\n",
    "\n",
    "corpus = [\n",
    " 'Crock Pot Pasta Never boil pasta again',\n",
    " 'Pasta Pomodoro Fresh ingredients Parmesan to taste'\n",
    "]\n",
    "\n",
    "tf_idf_vectorizer = TfIdfVectorizer(tokenizer, max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crock', 'pot', 'pasta', 'never', 'boil', 'again', 'pomodoro', 'fresh', 'ingredients', 'parmesan', 'to', 'taste']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.20078072972973776,\n",
       "  0.20078072972973776,\n",
       "  0.20351940787091974,\n",
       "  0.20078072972973776,\n",
       "  0.20078072972973776,\n",
       "  0.20078072972973776,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.10175970393545987,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.20078072972973776,\n",
       "  0.20078072972973776,\n",
       "  0.20078072972973776,\n",
       "  0.20078072972973776,\n",
       "  0.20078072972973776,\n",
       "  0.20078072972973776]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix = tf_idf_vectorizer.fit_transform(corpus, ordering = 'encounter')\n",
    "print(tf_idf_vectorizer.get_feature_names())\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crock', 'pot', 'pasta', 'never', 'boil', 'again', 'pomodoro', 'fresh', 'ingredients', 'parmesan', 'to', 'taste']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = tokenizer, max_features = None)\n",
    "count_matrix = vectorizer.fit_transform(corpus, ordering = 'encounter')\n",
    "print(vectorizer.get_feature_names())\n",
    "count_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Out: ['crock', 'pot', 'pasta', 'never', 'boil', 'again', 'pomodoro', 'fresh', 'ingredients', 'parmesan', 'to', 'taste']\n",
    "    \n",
    "Out: [[1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'https://meduza.io/paragraph/2020/10/26/skolko-oni-zarabatyvayut-i-chto-za-dannye-analiziruyut\\n'\\\n",
    "'Сколько они зарабатывают и что за данные анализируют? Кратко объясняем, зачем учиться на аналитика данных.\\n'\\\n",
    "'10:38, 26 октября 2020\\n'\\\n",
    "'Чтобы научиться собирать, обрабатывать и интерпретировать информацию. Эти навыки пригодятся в любой сфере — неважно, работаете ли вы в сервисе для заказа еды или крупном банке. Для этого нужно освоить базовый набор аналитика: SQL, математическую статистику, основы Python.\\n'\\\n",
    "'Чтобы принимать решения, опираясь на объективные данные, а не на интуицию и личный опыт. Цифры не врут, поэтому компании вроде Uber, Google, Tesla, Walmart, Netflix, eBay используют data-driven подход. Он позволяет составлять стратегии бизнеса и рекламных кампаний, выстраивать современную систему отчетности и не только.\\n'\\\n",
    "'Чтобы быть востребованным специалистом. Все больше компаний для эффективной работы нанимают аналитиков и готовы хорошо им платить. По запросу «аналитик данных» на hh.ru есть более 8,5 тысячи вакансий: новичок может рассчитывать на зарплату от 65 тысяч, опытный специалист — от 125 до 250 тысяч рублей.\\n'\\\n",
    "'Чтобы делать бизнес эффективнее. https://docs.google.com/spreadsheets/d/1p5zf4SpuyRl-enm_KGPf1qEPjYjnssfr924sZXBZq9h91mCLOU/edit#gid=2045843171 Согласно опросам, 65% компаний не в состоянии проанализировать имеющиеся данные, при этом 89% из них уверены, что если бы они могли это сделать, то получили бы конкурентное преимущество.\\n'\\\n",
    "'Чтобы решать интересные задачи. Может показаться, что аналитик данных не самая творческая профессия: ему каждый день приходится заниматься рутинным сбором и анализом. При этом работа может дать удивительные результаты — высветить проблемы, предсказать будущее, сделать явным то, что никто иначе бы не узнал.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://meduza.io/paragraph/2020/10/26/skolko-oni-zarabatyvayut-i-chto-za-dannye-analiziruyut\n",
      "Сколько они зарабатывают и что за данные анализируют? Кратко объясняем, зачем учиться на аналитика данных.\n",
      "10:38, 26 октября 2020\n",
      "Чтобы научиться собирать, обрабатывать и интерпретировать информацию. Эти навыки пригодятся в любой сфере — неважно, работаете ли вы в сервисе для заказа еды или крупном банке. Для этого нужно освоить базовый набор аналитика: SQL, математическую статистику, основы Python.\n",
      "Чтобы принимать решения, опираясь на объективные данные, а не на интуицию и личный опыт. Цифры не врут, поэтому компании вроде Uber, Google, Tesla, Walmart, Netflix, eBay используют data-driven подход. Он позволяет составлять стратегии бизнеса и рекламных кампаний, выстраивать современную систему отчетности и не только.\n",
      "Чтобы быть востребованным специалистом. Все больше компаний для эффективной работы нанимают аналитиков и готовы хорошо им платить. По запросу «аналитик данных» на hh.ru есть более 8,5 тысячи вакансий: новичок может рассчитывать на зарплату от 65 тысяч, опытный специалист — от 125 до 250 тысяч рублей.\n",
      "Чтобы делать бизнес эффективнее. https://docs.google.com/spreadsheets/d/1p5zf4SpuyRl-enm_KGPf1qEPjYjnssfr924sZXBZq9h91mCLOU/edit#gid=2045843171 Согласно опросам, 65% компаний не в состоянии проанализировать имеющиеся данные, при этом 89% из них уверены, что если бы они могли это сделать, то получили бы конкурентное преимущество.\n",
      "Чтобы решать интересные задачи. Может показаться, что аналитик данных не самая творческая профессия: ему каждый день приходится заниматься рутинным сбором и анализом. При этом работа может дать удивительные результаты — высветить проблемы, предсказать будущее, сделать явным то, что никто иначе бы не узнал.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'не', 'на', 'чтобы', 'что', 'данные', 'в', 'для', 'может', 'бы', 'они', 'аналитика', 'данных', 'компаний', 'от', 'тысяч', 'при', 'этом', 'сделать', 'то', 'сколько', 'зарабатывают', 'за', 'анализируют', 'кратко', 'объясняем', 'зачем', 'учиться', 'октября', 'научиться', 'собирать', 'обрабатывать', 'интерпретировать', 'информацию', 'эти', 'навыки', 'пригодятся', 'любой', 'сфере', 'неважно', 'работаете', 'ли', 'вы', 'сервисе', 'заказа', 'еды', 'или', 'крупном', 'банке', 'этого']\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07451886904375889, 0.0, 0.09369767387387762, 0.0, 0.10585244432680793, 0.12072868108108858, 0.0, 0.0, 0.0, 0.0, 0.13990748591120733, 0.13990748591120733, 0.13990748591120733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16693849311841827, 0.16693849311841827, 0.16693849311841827, 0.16693849311841827, 0.16693849311841827, 0.16693849311841827, 0.16693849311841827, 0.16693849311841827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.504077396776274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03992082270201369, 0.0, 0.0, 0.05019518243243444, 0.0, 0.0, 0.12935215830116634, 0.12935215830116634, 0.0, 0.0, 0.0, 0.07495043888100393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264, 0.08943133559915264], [0.335334910696915, 0.37539432848427184, 0.2810930216216329, 0.14054651081081646, 0.0, 0.18109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10161663960512576, 0.0, 0.2555391105651208, 0.1277695552825604, 0.0, 0.0, 0.0, 0.16463001965602989, 0.16463001965602989, 0.0, 0.0, 0.0, 0.0, 0.19078293533346455, 0.3815658706669291, 0.3815658706669291, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.09625495602160816, 0.0, 0.10811270062370495, 0.12213743576170147, 0.13930232432433298, 0.13930232432433298, 0.0, 0.0, 0.27860464864866596, 0.16143171451293153, 0.0, 0.0, 0.16143171451293153, 0.0, 0.0, 0.16143171451293153, 0.16143171451293153, 0.16143171451293153, 0.16143171451293153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07984164540402738, 0.17875920404012943, 0.0, 0.10039036486486888, 0.22682666641458843, 0.0, 0.0, 0.0, 0.2587043166023327, 0.12935215830116634, 0.0, 0.0, 0.14990087776200786, 0.0, 0.0, 0.0, 0.14990087776200786, 0.14990087776200786, 0.14990087776200786, 0.14990087776200786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(' ', punctuation = string.punctuation+'—')\n",
    "\n",
    "vectorizer = TfIdfVectorizer(tokenizer = tokenizer, max_features = 50)\n",
    "texts = text.strip().split('\\n')\n",
    "count_matrix = vectorizer.fit_transform(texts, ordering = 'counts', as_array = True)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max_features = 2, ordering = counts\n",
      "\n",
      "['pasta', 'crock']\n",
      "[[2, 1], [1, 0]]\n",
      "\n",
      "max_features = 2, ordering = encounter\n",
      "\n",
      "['crock', 'pasta']\n",
      "[[1, 2], [0, 1]]\n",
      "\n",
      "max_features = 2, ordering = alphabetical\n",
      "\n",
      "['crock', 'pasta']\n",
      "[[1, 2], [0, 1]]\n",
      "\n",
      "max_features = 8, ordering = counts\n",
      "\n",
      "['pasta', 'crock', 'pot', 'never', 'boil', 'again', 'pomodoro', 'fresh']\n",
      "[[2, 1, 1, 1, 1, 1, 0, 0], [1, 0, 0, 0, 0, 0, 1, 1]]\n",
      "\n",
      "max_features = 8, ordering = encounter\n",
      "\n",
      "['crock', 'pot', 'pasta', 'never', 'boil', 'again', 'pomodoro', 'fresh']\n",
      "[[1, 1, 2, 1, 1, 1, 0, 0], [0, 0, 1, 0, 0, 0, 1, 1]]\n",
      "\n",
      "max_features = 8, ordering = alphabetical\n",
      "\n",
      "['again', 'boil', 'crock', 'fresh', 'never', 'pasta', 'pomodoro', 'pot']\n",
      "[[1, 1, 1, 0, 1, 2, 0, 1], [0, 0, 0, 1, 0, 1, 1, 0]]\n",
      "\n",
      "max_features = 20, ordering = counts\n",
      "\n",
      "['pasta', 'crock', 'pot', 'never', 'boil', 'again', 'pomodoro', 'fresh', 'ingredients', 'parmesan', 'to', 'taste']\n",
      "[[2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]\n",
      "\n",
      "max_features = 20, ordering = encounter\n",
      "\n",
      "['crock', 'pot', 'pasta', 'never', 'boil', 'again', 'pomodoro', 'fresh', 'ingredients', 'parmesan', 'to', 'taste']\n",
      "[[1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1]]\n",
      "\n",
      "max_features = 20, ordering = alphabetical\n",
      "\n",
      "['again', 'boil', 'crock', 'fresh', 'ingredients', 'never', 'parmesan', 'pasta', 'pomodoro', 'pot', 'taste', 'to']\n",
      "[[1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0], [0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(' ')\n",
    "\n",
    "corpus = [\n",
    " 'Crock Pot Pasta Never boil pasta again',\n",
    " 'Pasta Pomodoro Fresh ingredients Parmesan to taste'\n",
    "]\n",
    "\n",
    "for max_features, ordering in itertools.product([2, 8, 20], ['counts', 'encounter', 'alphabetical']):\n",
    "    print(f'\\nmax_features = {max_features}, ordering = {ordering}\\n')\n",
    "\n",
    "    vectorizer = CountVectorizer(tokenizer = tokenizer, max_features = max_features)\n",
    "    count_matrix = vectorizer.fit_transform(corpus, ordering = ordering)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max_features = 2, ordering = counts\n",
      "\n",
      "['pasta', 'crock']\n",
      "[[0.4748786183654794, 0.4684883693693881], [0.7123179275482191, 0.0]]\n",
      "\n",
      "max_features = 2, ordering = encounter\n",
      "\n",
      "['crock', 'pasta']\n",
      "[[0.4684883693693881, 0.4748786183654794], [0.0, 0.7123179275482191]]\n",
      "\n",
      "max_features = 2, ordering = alphabetical\n",
      "\n",
      "['crock', 'pasta']\n",
      "[[0.4684883693693881, 0.4748786183654794], [0.0, 0.7123179275482191]]\n",
      "\n",
      "max_features = 8, ordering = counts\n",
      "\n",
      "['pasta', 'crock', 'pot', 'never', 'boil', 'again', 'pomodoro', 'fresh']\n",
      "[[0.20351940787091974, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.0, 0.0], [0.2374393091827397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4684883693693881, 0.4684883693693881]]\n",
      "\n",
      "max_features = 8, ordering = encounter\n",
      "\n",
      "['crock', 'pot', 'pasta', 'never', 'boil', 'again', 'pomodoro', 'fresh']\n",
      "[[0.20078072972973776, 0.20078072972973776, 0.20351940787091974, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.0, 0.0], [0.0, 0.0, 0.2374393091827397, 0.0, 0.0, 0.0, 0.4684883693693881, 0.4684883693693881]]\n",
      "\n",
      "max_features = 8, ordering = alphabetical\n",
      "\n",
      "['again', 'boil', 'crock', 'fresh', 'never', 'pasta', 'pomodoro', 'pot']\n",
      "[[0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.0, 0.20078072972973776, 0.20351940787091974, 0.0, 0.20078072972973776], [0.0, 0.0, 0.0, 0.4684883693693881, 0.0, 0.2374393091827397, 0.4684883693693881, 0.0]]\n",
      "\n",
      "max_features = 20, ordering = counts\n",
      "\n",
      "['pasta', 'crock', 'pot', 'never', 'boil', 'again', 'pomodoro', 'fresh', 'ingredients', 'parmesan', 'to', 'taste']\n",
      "[[0.20351940787091974, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10175970393545987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776]]\n",
      "\n",
      "max_features = 20, ordering = encounter\n",
      "\n",
      "['crock', 'pot', 'pasta', 'never', 'boil', 'again', 'pomodoro', 'fresh', 'ingredients', 'parmesan', 'to', 'taste']\n",
      "[[0.20078072972973776, 0.20078072972973776, 0.20351940787091974, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.10175970393545987, 0.0, 0.0, 0.0, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.20078072972973776]]\n",
      "\n",
      "max_features = 20, ordering = alphabetical\n",
      "\n",
      "['again', 'boil', 'crock', 'fresh', 'ingredients', 'never', 'parmesan', 'pasta', 'pomodoro', 'pot', 'taste', 'to']\n",
      "[[0.20078072972973776, 0.20078072972973776, 0.20078072972973776, 0.0, 0.0, 0.20078072972973776, 0.0, 0.20351940787091974, 0.0, 0.20078072972973776, 0.0, 0.0], [0.0, 0.0, 0.0, 0.20078072972973776, 0.20078072972973776, 0.0, 0.20078072972973776, 0.10175970393545987, 0.20078072972973776, 0.0, 0.20078072972973776, 0.20078072972973776]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(' ')\n",
    "\n",
    "corpus = [\n",
    " 'Crock Pot Pasta Never boil pasta again',\n",
    " 'Pasta Pomodoro Fresh ingredients Parmesan to taste'\n",
    "]\n",
    "\n",
    "for max_features, ordering in itertools.product([2, 8, 20], ['counts', 'encounter', 'alphabetical']):\n",
    "    print(f'\\nmax_features = {max_features}, ordering = {ordering}\\n')\n",
    "\n",
    "    vectorizer = TfIdfVectorizer(tokenizer = tokenizer, max_features = max_features)\n",
    "    out_matrix = vectorizer.fit_transform(corpus, ordering = ordering)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(out_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
